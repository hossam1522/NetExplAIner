from netexplainer.llm import LLM_GEMINI
from netexplainer.dataset import Dataset
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import ChatPromptTemplate

class Evaluator:
    """
    Class for evaluating the LLM
    """
    def __init__(self, dataset: Dataset):
        """
        Initialize the evaluator object with the file provided

        Args:
            data_path (str): The path of the file to process
            questions_path (str): The path of the questions file
        """
        self.dataset = dataset
        self.llm = LLM_GEMINI(dataset.processed_file)

    def evaluate_subquestions(self, question: str, subquestions: list) -> None:
        """
        Evaluate the subquestions obtained from the LLM
        """
        template = """Here is a question and the subquestions that were generated:
        Question: {question}
        Subquestions: {subquestions_LLM}
        Compare these to the subquestions that were provided:
        {subquestions}
        Are the subquestions generated by the LLM similar to the subquestions provided?
        You ONLY can answer YES/APPROX/NO"""
        prompt = ChatPromptTemplate.from_template(template)
        chain = (
            prompt
            | self.llm.model
            | StrOutputParser()
        )
        answer = chain.invoke({"question": question, "subquestions_LLM": subquestions, "subquestions": self.dataset.questions_subquestions[question]})
        return answer
