from netexplainer.llm import LLM_GEMINI
from netexplainer.dataset import Dataset
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import ChatPromptTemplate

class Evaluator:
    """
    Class for evaluating the LLM
    """
    def __init__(self, dataset: Dataset):
        """
        Initialize the evaluator object with the file provided

        Args:
            data_path (str): The path of the file to process
            questions_path (str): The path of the questions file
        """
        self.dataset = dataset
        self.llm = LLM_GEMINI(dataset.processed_file)

    def evaluate_subquestions(self, question: str, subquestions: list) -> str:
        """
        Evaluate the subquestions obtained from the LLM

        Args:
            question (str): The question to process
            subquestions (list): The list of sub-questions

        Returns:
            str: The evaluation result
        """
        template = """Here is a question and the subquestions that were generated:
        Question: {question}
        Subquestions: {subquestions_LLM}
        Compare these to the subquestions that were provided:
        {subquestions}
        Are the subquestions generated by the LLM similar to the subquestions provided?
        You ONLY can answer YES/APPROX/NO"""
        prompt = ChatPromptTemplate.from_template(template)
        chain = (
            prompt
            | self.llm.model
            | StrOutputParser()
        )
        answer = chain.invoke({"question": question, "subquestions_LLM": subquestions, "subquestions": self.dataset.questions_subquestions[question]})
        return answer

    def evaluate_answers(self, question: str, answer: str) -> str:
        """
        Evaluate the answers obtained from the LLM

        Args:
            question (str): The question to process
            answer (str): The answer to process

        Returns:
            str: The evaluation result
        """
        template = """Here is a question and the answer that was generated:
        Question: {question}
        Answers: {answer_LLM}
        Compare it with the answer that was provided:
        {answer}
        Are the answers generated by the LLM similar to the answer provided?
        You ONLY can answer YES/NO"""
        prompt = ChatPromptTemplate.from_template(template)
        chain = (
            prompt
            | self.llm.model
            | StrOutputParser()
        )
        answer = chain.invoke({"question": question, "answer_LLM": answer, "answer": self.dataset.questions_answers[question]})
        return answer
