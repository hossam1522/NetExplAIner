import os
import time
import re
import logging
from pathlib import Path
import plotly.express as px
import plotly.graph_objects as go
from netexplainer.dataset import Dataset
from netexplainer.llm import models
from netexplainer.dataset import Dataset
from netexplainer.logger import configure_logger
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import ChatPromptTemplate

configure_logger(name="evaluator", filepath=Path(__file__).parent / "data/evaluation/netexplainer.log")
logger = logging.getLogger("evaluator")
QUESTIONS_PATH = "netexplainer/data/questions.yaml"


class Evaluator:
    """
    Class for evaluating the LLM
    """
    def evaluate_subquestions(self, question: str, subquestions: list, dataset: Dataset) -> str:
        """
        Evaluate the subquestions obtained from the LLM

        Args:
            question (str): The question to process
            subquestions (list): The list of sub-questions

        Returns:
            str: The evaluation result
        """
        template = """Here is a question and the subquestions that were generated by LLM:
        Question: "{question}"
        Subquestions: "{subquestions_LLM}"
        Compare these to the subquestions that were provided:
        "{subquestions}"
        Are the subquestions generated by the LLM similar to the subquestions provided?
        You ONLY can answer with a number, indicating the percentage of similarity."""
        prompt = ChatPromptTemplate.from_template(template)
        llm = models["gemma-3-27b"][0](dataset.processed_file)
        chain = (
            prompt
            | llm.llm
            | StrOutputParser()
        )
        answer = chain.invoke({"question": question, "subquestions_LLM": subquestions, "subquestions": dataset.questions_subquestions[question]})
        logger.debug(f"Question: {question}, Subquestions LLM: {subquestions}, Subquestions: {dataset.questions_subquestions[question]}, Similarity: {answer}")
        return answer

    def evaluate_answer(self, question: str, answer_llm: str, dataset: Dataset) -> str:
        """
        Evaluate the answers obtained from the LLM

        Args:
            question (str): The question to process
            answer_llm (str): The answer generated by the LLM
            dataset (Dataset): The dataset containing the questions and answers

        Returns:
            str: The evaluation result
        """
        template = """Here is a question and the answer that was generated by LLM:
        Question: "{question}"
        Answer LLM: "{answer_LLM}"
        Compare it with the answer that was provided:
        "{answer}"
        Is the answer generated by the LLM almost correct compared to the answer provided?
        You ONLY can answer YES/NO"""
        prompt = ChatPromptTemplate.from_template(template)
        llm = models["gemma-3-27b"][0](dataset.processed_file)
        chain = (
            prompt
            | llm.llm
            | StrOutputParser()
        )
        answer = chain.invoke({"question": question, "answer_LLM": answer_llm, "answer": dataset.questions_answers[question]})
        logger.debug(f"Question: {question}, Answer LLM: {answer_llm}, Answer: {dataset.questions_answers[question]}, Comparison: {answer}")
        return answer

    def evaluate(self, models_to_evaluate: list, tools: bool = False) -> None:
        """
        Evaluates the models without using any tools.

        Args:
            models_to_evaluate (list): List of models to evaluate.
            tools (bool): Whether to use tools or not.
        """
        for model in models_to_evaluate:
            all_results = []

            for file in os.listdir("netexplainer/data/cleaned/"):
                if file.endswith(".txt"):
                    continue

                try:
                    logger.debug(f"Processing file: {file} with model: {model}")
                    dataset = Dataset(os.path.join("netexplainer/data/cleaned/", file), QUESTIONS_PATH, models[f"{model}"][1])
                    llm = models[f"{model}"][0](dataset.processed_file, tools=tools)

                    for question in dataset.questions_subquestions.keys():
                        logger.debug(f"Processing question: {question} with model: {model}")
                        for _ in range(10):
                            logger.debug(f"Attempting to process question: {question} with model: {model}, attempt: {_ + 1}")
                            try:
                                subquestions = llm.get_subquestions(question)

                                answers = []
                                for subquestion in subquestions:
                                    time.sleep(2.5)
                                    answer = llm.answer_subquestion(subquestion)
                                    answers.append(answer)

                                time.sleep(2.5)
                                final_answer = llm.get_final_answer(question, subquestions, answers)

                                try:
                                    time.sleep(2)
                                    subquestions_eval = self.evaluate_subquestions(question, subquestions, dataset)
                                except Exception as e:
                                    logger.error(f"Error evaluating subquestions: {e}")
                                    subquestions_eval = "ERROR"

                                try:
                                    time.sleep(2)
                                    answers_eval = self.evaluate_answer(question, final_answer, dataset)
                                except Exception as e:
                                    logger.error(f"Error evaluating answers: {e}")
                                    answers_eval = "PROBLEM"

                                if subquestions_eval != "ERROR" and answers_eval != "PROBLEM":
                                    break

                            except Exception as e:
                                logger.error(f"Error processing question {question} in file {file}: {e}")
                                subquestions_eval = "ERROR"
                                answers_eval = "PROBLEM"

                        all_results.append({
                            "model": model,
                            "file": file,
                            "question": question,
                            "subquestions_eval": subquestions_eval,
                            "answer_eval": answers_eval
                        })

                        if tools:
                            logger.info(f"Model: {model}_tools, File: {file}, Question: {question}, Subquestions Eval: {subquestions_eval}, Answer Eval: {answers_eval}")
                        else:
                            logger.info(f"Model: {model}, File: {file}, Question: {question}, Subquestions Eval: {subquestions_eval}, Answer Eval: {answers_eval}")

                except Exception as e:
                    logger.error(f"Error processing file {file} with model {model}: {e}")

            logger.debug("Generating pie charts")
            self.generate_pie_charts(all_results, tools)
            logger.debug("Generating bar charts")
            self.generate_bar_charts(all_results, tools)
            logger.debug("Generating model subquestions charts")
            self.generate_model_subquestions_chart(all_results, tools)

    def generate_model_subquestions_chart(self, results: list, tools: bool = False) -> None:
        """
        Generate radar charts for the subquestions similarity evaluation.

        Args:
            results (list): List of evaluation results.
        """
        from collections import defaultdict

        model_data = defaultdict(lambda: defaultdict(list))

        for result in results:
            model = result["model"]
            question = result["question"]
            sub_eval = result["subquestions_eval"].replace('%', '') if isinstance(result["subquestions_eval"], str) else result["subquestions_eval"]

            if not question or sub_eval == "ERROR":
                continue

            try:
                similarity = float(sub_eval)
                model_data[model][question].append(similarity)
            except:
                continue

        for model, questions in model_data.items():
            sorted_questions = sorted(
                questions.keys(),
                key=lambda x: int(re.search(r'\d+', x).group()) if re.search(r'\d+', x) else 0
            )

            avg_values = []
            valid_questions = []

            for q in sorted_questions:
                avg = sum(questions[q]) / len(questions[q])
                avg_values.append(round(avg, 2))
                valid_questions.append("Question " + str(len(valid_questions) + 1))

            fig = go.Figure(data=go.Scatterpolar(
                r=avg_values,
                theta=valid_questions,
                fill='toself',
                line=dict(color='royalblue'),
                name="Similarity"
            ))

            if tools:
                title = f"Subquestions similarity: {model} with tools"
            else:
                title = f"Subquestions similarity: {model}"

            fig.update_layout(
                polar=dict(
                    radialaxis=dict(
                        visible=True,
                        range=[0, 100],
                        tickfont=dict(size=10)
                    ),
                    angularaxis=dict(tickfont=dict(size=12))
                ),
                title=title,
                showlegend=True,
                width=800,
                height=600
            )

            dir_path = ""
            if tools:
                dir_path = f"netexplainer/data/evaluation/{model}_tools/"
            else:
                dir_path = f"netexplainer/data/evaluation/{model}/"
            os.makedirs(dir_path, exist_ok=True)

            with open(f"{dir_path}subquestions_similarity.txt", "w") as f:
                f.write(f"Model: {model}\n")
                f.write(f"Subquestions similarity:\n")
                for question in sorted_questions:
                    f.write(f"{question}: {avg_values[sorted_questions.index(question)]}\n")
                    logger.info(f"Model: {model}, Question: {question}, Similarity: {avg_values[sorted_questions.index(question)]}")

            fig.write_image(f"{dir_path}radar_subquestions_similarity.png")

    def generate_bar_charts(self, results: list, tools: bool = False) -> None:
        """
        Generate grouped bar charts for correct/incorrect answers per question for each model.
        """
        from collections import defaultdict

        model_question_data = defaultdict(lambda: defaultdict(lambda: {'YES': 0, 'NO': 0}))

        for result in results:
            model = result["model"]
            question = result["question"]
            eval = result["answer_eval"]

            if eval in ['YES', 'NO']:
                model_question_data[model][question][eval] += 1

        for model, questions in model_question_data.items():
            sorted_questions = sorted(
                questions.keys(),
                key=lambda x: int(re.search(r'\d+', x).group()) if re.search(r'\d+', x) else 0
            )

            yes_values = []
            no_values = []
            q_labels = []

            for q in sorted_questions:
                yes_values.append(questions[q]['YES'])
                no_values.append(questions[q]['NO'])
                q_labels.append("Question " + str(len(q_labels) + 1))

            fig = go.Figure(data=[
                go.Bar(name='Correct (YES)', x=q_labels, y=yes_values, marker_color='#4CAF50'),
                go.Bar(name='Incorrect (NO)', x=q_labels, y=no_values, marker_color='#F44336')
            ])

            if tools:
                title = f"Correct and incorrect answers by question: {model} with tools"
            else:
                title = f"Correct and incorrect answers by question: {model}"

            fig.update_layout(
                barmode='group',
                title=title,
                xaxis_title="Questions",
                yaxis_title="Count",
                legend=dict(orientation="h", yanchor="bottom", y=1.02),
                width=1200,
                height=600,
                margin=dict(t=60)
            )

            dir_path = ""
            if tools:
                dir_path = f"netexplainer/data/evaluation/{model}_tools/"
            else:
                dir_path = f"netexplainer/data/evaluation/{model}/"
            os.makedirs(dir_path, exist_ok=True)

            with open(f"{dir_path}answers.txt", "w") as f:
                f.write(f"Model: {model}\n")
                f.write(f"Correct and incorrect answers:\n")
                for question in sorted_questions:
                    f.write(f"{question}: Correct: {questions[question]['YES']}, Incorrect: {questions[question]['NO']}\n")
                    logger.info(f"Model: {model}, Question: {question}, Correct: {questions[question]['YES']}, Incorrect: {questions[question]['NO']}")

            fig.write_image(f"{dir_path}grouped_bar_answers.png")

    def generate_pie_charts(self, results: list, tools: bool = False) -> None:
        """
        Generate pie charts from the evaluation results.

        Args:
            results (list): List of evaluation results.
        """
        from collections import defaultdict, OrderedDict

        model_data = defaultdict(list)
        for result in results:
            model_data[result["model"]].append(result["answer_eval"])

        for model, evaluations in model_data.items():
            counts = OrderedDict([
                ("Correct (YES)", 0),
                ("Incorrect (NO)", 0),
                ("Problematic (PROBLEM)", 0)
            ])

            for eval in evaluations:
                if eval == "YES":
                    counts["Correct (YES)"] += 1
                elif eval == "NO":
                    counts["Incorrect (NO)"] += 1
                else:
                    counts["Problematic (PROBLEM)"] += 1

            total = sum(counts.values())
            if total == 0:
                continue

            labels = list(counts.keys())
            values = [round((count/total)*100, 2) for count in counts.values()]

            if tools:
                title = f"Correct and incorrect answers: {model} with tools"
            else:
                title = f"Correct and incorrect answers: {model}"

            fig = px.pie(
                names=labels,
                values=values,
                title=title,
                color_discrete_sequence=px.colors.qualitative.Pastel
            )

            dir_path = ""
            if tools:
                dir_path = f"netexplainer/data/evaluation/{model}_tools/"
            else:
                dir_path = f"netexplainer/data/evaluation/{model}/"
            os.makedirs(dir_path, exist_ok=True)

            with open(f"{dir_path}answers_pie_chart.txt", "w") as f:
                f.write(f"Model: {model}\n")
                f.write(f"Correct and incorrect answers:\n")
                for label, value in zip(labels, values):
                    f.write(f"{label}: {value}%\n")
                    logger.info(f"Model: {model}, {label}: {value}%")

            fig.write_image(f"{dir_path}answers_pie_chart.png")
